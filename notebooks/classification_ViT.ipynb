{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/coartix/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_hip.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
                        "  warn(\n"
                    ]
                }
            ],
            "source": [
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import os\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "os.chdir('../')\n",
                "from utils import Config\n",
                "from models.ViT import vit_tiny"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"configs/cifar10_base.yml\") as f:\n",
                "        yml_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
                "config = Config(yml_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Files already downloaded and verified\n"
                    ]
                }
            ],
            "source": [
                "dataset_original = torchvision.datasets.CIFAR10(\n",
                "    root=config.original_dataset_path,\n",
                "    train=False,\n",
                "    download=True,\n",
                "    transform=transforms.ToTensor()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Testing ViT on classification task"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from models.ViT import vit_tiny\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ViT_small(\n",
                            "  (patch_embed): PatchEmbed(\n",
                            "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
                            "  )\n",
                            "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
                            "  (blocks): ModuleList(\n",
                            "    (0-4): 5 x Block(\n",
                            "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
                            "      (attn): SelfAttention(\n",
                            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (k): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (v): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
                            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
                            "      )\n",
                            "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
                            "      (mlp): MLP(\n",
                            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
                            "        (act): GELU(approximate='none')\n",
                            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
                            "        (drop): Dropout(p=0.0, inplace=False)\n",
                            "      )\n",
                            "      (drop_layer): DropLayer()\n",
                            "    )\n",
                            "    (5): Block(\n",
                            "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
                            "      (attn): SelfAttention(\n",
                            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (k): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (v): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
                            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
                            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
                            "      )\n",
                            "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
                            "      (mlp): MLP(\n",
                            "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
                            "        (act): GELU(approximate='none')\n",
                            "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
                            "        (drop): Dropout(p=0.0, inplace=False)\n",
                            "      )\n",
                            "      (drop_layer): Identity()\n",
                            "    )\n",
                            "  )\n",
                            "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
                            "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
                            ")"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "clf_model = vit_tiny(patch_size=4, embed_dim=192, depth=6, num_classes=10, img_size=[32],\n",
                "  drop_rate=0.2, attn_drop_rate=0.2, drop_layer_rate=0.2, num_heads=6)\n",
                "clf_model.cuda()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Creating dataloader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_size = int(0.8 * len(dataset_original))\n",
                "test_size = len(dataset_original) - train_size\n",
                "train_dataset, test_dataset = torch.utils.data.random_split(dataset_original, [train_size, test_size])\n",
                "\n",
                "train_dataloader = torch.utils.data.DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=config.batch_size,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "test_dataloader = torch.utils.data.DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=config.batch_size,\n",
                "    shuffle=False\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Simple training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch: 0\n",
                        "Loss: 2.347198123931885\n",
                        "Loss: 2.2167662239074706\n",
                        "Epoch: 1\n",
                        "Loss: 2.040811152458191\n",
                        "Loss: 2.002656092643738\n",
                        "Epoch: 2\n",
                        "Loss: 1.9329124116897582\n",
                        "Loss: 1.8971667718887328\n",
                        "Epoch: 3\n",
                        "Loss: 1.880981822013855\n",
                        "Loss: 1.8420228624343873\n",
                        "Epoch: 4\n",
                        "Loss: 1.8387811803817748\n",
                        "Loss: 1.8184457969665528\n",
                        "Epoch: 5\n",
                        "Loss: 1.763266043663025\n",
                        "Loss: 1.7365613269805908\n",
                        "Epoch: 6\n",
                        "Loss: 1.750727548599243\n",
                        "Loss: 1.7459416007995605\n",
                        "Epoch: 7\n",
                        "Loss: 1.6914555358886718\n",
                        "Loss: 1.7028889989852904\n",
                        "Epoch: 8\n",
                        "Loss: 1.6671459197998046\n",
                        "Loss: 1.6606989049911498\n",
                        "Epoch: 9\n",
                        "Loss: 1.6259217596054076\n",
                        "Loss: 1.6596532583236694\n",
                        "Epoch: 10\n",
                        "Loss: 1.6262207746505737\n",
                        "Loss: 1.6141299772262574\n",
                        "Epoch: 11\n",
                        "Loss: 1.6079702854156495\n",
                        "Loss: 1.5789149522781372\n",
                        "Epoch: 12\n",
                        "Loss: 1.604694504737854\n",
                        "Loss: 1.609003086090088\n",
                        "Epoch: 13\n",
                        "Loss: 1.5666968774795533\n",
                        "Loss: 1.5651464653015137\n",
                        "Epoch: 14\n",
                        "Loss: 1.5299469089508058\n",
                        "Loss: 1.5822199201583862\n",
                        "Epoch: 15\n",
                        "Loss: 1.5323998880386354\n",
                        "Loss: 1.5439596462249756\n",
                        "Epoch: 16\n",
                        "Loss: 1.5201569032669067\n",
                        "Loss: 1.5442328882217407\n",
                        "Epoch: 17\n",
                        "Loss: 1.5138950061798095\n",
                        "Loss: 1.4674993705749513\n",
                        "Epoch: 18\n",
                        "Loss: 1.4765195274353027\n",
                        "Loss: 1.5157141494750976\n",
                        "Epoch: 19\n",
                        "Loss: 1.482356195449829\n",
                        "Loss: 1.4722854232788085\n",
                        "Finished Training\n"
                    ]
                }
            ],
            "source": [
                "epochs = 20\n",
                "optimizer = optim.Adam(clf_model.parameters())\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    print(\"Epoch:\", epoch)\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(train_dataloader, 0):\n",
                "        inputs, label = data\n",
                "        inputs, label = inputs.cuda(), label.cuda()\n",
                "        output = clf_model(inputs)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        loss = F.cross_entropy(output, label)\n",
                "        loss.backward()\n",
                "\n",
                "        optimizer.step()\n",
                "\n",
                "        running_loss += loss.item()\n",
                "        if i % 25 == 24:\n",
                "            print(\"Loss:\", running_loss / 25)\n",
                "            running_loss = 0.0\n",
                "\n",
                "print(\"Finished Training\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy of the network on the test images: 43.7%\n"
                    ]
                }
            ],
            "source": [
                "correct = 0\n",
                "total = 0\n",
                "\n",
                "with torch.no_grad():\n",
                "    for data in test_dataloader:\n",
                "        input, label = data\n",
                "        input, label = input.cuda(), label.cuda()\n",
                "        output = clf_model(input)\n",
                "        _, predicted = torch.max(output.data, 1)\n",
                "        total += label.size(0)\n",
                "        correct += (predicted == label).sum().item()\n",
                "\n",
                "print(f\"Accuracy of the network on the test images: {100 * correct / total}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "torch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
